{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of \n",
    "\n",
    "## <center>  PseudoEdgeNet: Nuclei Segemtnation only with Point Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* kernel size of PseudoEdgeNet\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info\n",
    "\n",
    "**TCGA data set:**\n",
    "\n",
    "* 30 H&E stained whole slide images from TCGA, have 7 organs, one WSI per patient, come from 18 hospitals.\n",
    "\n",
    "* Then crop 30 sub-images, each size is 1000 * 1000, and keep only one cropped image per WSI and patient\n",
    "\n",
    "* H&E stained, captured at 40X magnification\n",
    "\n",
    "**TASK:**\n",
    "\n",
    "* Instance segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotation\n",
    "\n",
    "* Pixel coordinates of annoated nuclear boundaries\n",
    "\n",
    "* For over-lapping nuclei, assign multi-nuclear pixel to the largest nucleus containing that pixel\n",
    "\n",
    "* How to deal with float coordinates?\n",
    "\n",
    "    * round them, cast to int\n",
    "\n",
    "* source:\n",
    "\n",
    "    * https://stackoverflow.com/questions/10735817/how-to-deal-with-floating-point-coordinate-values-in-image\n",
    "\n",
    "    * https://stackoverflow.com/questions/58991754/how-to-use-float-number-as-opencv-image-pixel-coordinates\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Point Annotation(How to find centroid of any shape)\n",
    "\n",
    "**source:**\n",
    "\n",
    "* https://stackoverflow.com/questions/23020659/fastest-way-to-calculate-the-centroid-of-a-set-of-coordinate-tuples-in-python-wi\n",
    "\n",
    "* https://www.learnopencv.com/find-center-of-blob-centroid-using-opencv-cpp-python/\n",
    "\n",
    "* https://www.pyimagesearch.com/2016/02/01/opencv-center-of-contour/\n",
    "\n",
    "**centroid definition in paper**:\n",
    "\n",
    "* center of mass of each nucleus instatnce mask\n",
    "\n",
    "**centroid calculation:**\n",
    "\n",
    "* centroid = $\\frac{\\sum_{i=0}^n x_i}{n}$, centroid of any shape is the average of contour coordinates, where $x_i$ is the coordinate, n is the length of contour or number of pixel of contour in here.\n",
    "\n",
    "**Contour structure in Opencv**:\n",
    "\n",
    "* Contour is a list, length is the number of nuclei in image\n",
    "\n",
    "* for each element, it is a contour of a single blob, each element is a numpy array has this shape: (number of pixel of contour, 1, 2)\n",
    "\n",
    "* for each pixel of contour, it is a numpy array with shape of (1, 2)\n",
    "\n",
    "**In my implementation:**\n",
    "\n",
    "* construct contour data structure by groud truth, then use `cv.moment()` find centroid\n",
    "\n",
    "### Generate voronoi boundary\n",
    "\n",
    "* use opencv `cv.subdiv()` to compute voronoi boundary, then use `cv.polylines()` to assign value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input:\n",
    "\n",
    "* image I\n",
    "\n",
    "### Loss Function:\n",
    "\n",
    "* old: $\\mathcal{L}(I, P, f, g) = \\mathcal{L}_{ce}(f(I), P) + \\lambda \\cdot |s(f(I)) - g(I)|$\n",
    "\n",
    "* new: $\\mathcal{L}(I, P, f, g) = \\mathcal{L}_{ce}(f(I), P) + \\lambda \\cdot |s(f(I)) - g(I) \\otimes h(I)|$\n",
    "\n",
    "### $\\mathcal{L}_{ce}$:\n",
    "\n",
    "* pixel-averaged cross-entropy loss\n",
    "\n",
    "* positive labels**(with weight 1.0)** are given to the pixels corresponding to point annotations,\n",
    "\n",
    "* negative labels**(with weight 0.1)** are assigned to pixels on Voronoi boundaries that can be obtained by distance transform with point annotations\n",
    "\n",
    "* ignore all pixel that are not annotated\n",
    "\n",
    "### $f(I)$:\n",
    "\n",
    "* employ a Feature Pyramid Network \n",
    "\n",
    "* with ResNet-50 backbone followed by a sigmoid layer as the segmentation network\n",
    "\n",
    "* threshold to determine positive pixels from f(I) is 0.5\n",
    "\n",
    "* more detail: http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf\n",
    "\n",
    "### P:\n",
    "\n",
    "* point annotation\n",
    "\n",
    "### $\\lambda$:\n",
    "\n",
    "* scaling constant 1.0\n",
    "\n",
    "### $s()$:\n",
    "\n",
    "* (x, y) directional Sobel filter\n",
    "\n",
    "### $g(I)$:\n",
    "\n",
    "* EdgeNet to detect edge, Jointly learn in training time, not used in inference time\n",
    "\n",
    "* Structure:\n",
    "\n",
    "    * 4 CNN layer with 64 filters followed by batch normalization and ReLU\n",
    "    \n",
    "    * kernel size ? 3 * 3?\n",
    "\n",
    "    * except for the last layer, produces a two-channel output represent (x, y)-directional Sobel edge maps, one for x, another for y\n",
    "\n",
    "* small g is good, with 2,4,6,8 conv layer \n",
    "\n",
    "### $h(I)$:\n",
    "\n",
    "* use FPN with a Resnet-18 backbone\n",
    "\n",
    "* stack a sigmoid as an output layer\n",
    "\n",
    "* $\\otimes$ represent element-wise multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> f--segmentation Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone\n",
    "\n",
    "**ResNet50:**\n",
    "\n",
    "* [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)(Original)\n",
    "\n",
    "* [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027)(improved)\n",
    "\n",
    "* I thought pre-trained network have no help to nuclei segmentation, therefore, pre-trained ResNet50 is not used\n",
    "\n",
    "* original paper use initial version of ResNet\n",
    "\n",
    "**FPN:**\n",
    "\n",
    "* In decode process, add a crop layer after each upsampling layer to maintain output shape.\n",
    "\n",
    "* crop left column and top row \n",
    "\n",
    "* source: https://stackoverflow.com/questions/46035581/how-to-ensure-caffe-segmentation-network-output-size-is-the-same-as-input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Color jittering\n",
    "    * https://stackoverflow.com/questions/35152636/random-flipping-and-rgb-jittering-slight-value-change-of-image\n",
    "\n",
    "* Gaussian blurring\n",
    "\n",
    "* Gaussian noise injection\n",
    "    * https://answers.opencv.org/question/79758/how-to-add-gaussian-noise-in-all-channels-of-an-image-with-randn/\n",
    "\n",
    "* rotation\n",
    "    * https://docs.opencv.org/master/dd/d52/tutorial_js_geometric_transformations.html\n",
    "* horizontal filp\n",
    "    * https://docs.opencv.org/master/d2/de8/group__core__array.html#gaca7be533e3dac7feb70fc60635adf441\n",
    "    \n",
    "* vertical flip\n",
    "    * https://docs.opencv.org/master/d2/de8/group__core__array.html#gaca7be533e3dac7feb70fc60635adf441\n",
    "* affine transformation\n",
    "\n",
    "* elastic deformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer:\n",
    "\n",
    "* Adam, with an initial learning rate of 0.001,\n",
    "\n",
    "* learning rate is halved when the average loss per epoch does not decrease for current 5 epochs\n",
    "\n",
    "* patience = 4\n",
    "\n",
    "* plateau:\n",
    "\n",
    "    * https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/lr_scheduling/#reduce-on-loss-plateau-decay-patience0-factor01\n",
    "\n",
    "    * https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "### Evalutaion metric:\n",
    "\n",
    "* Intersection over union (IoU)\n",
    "    * https://www.kaggle.com/c/data-science-bowl-2018/discussion/51553\n",
    "    * https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/63044"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> First trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "* with PseudoEdgeNet, without attention Module\n",
    "\n",
    "* with all augmentation except affine and elastic deformation\n",
    "\n",
    "* batch_size = 4\n",
    "\n",
    "* epoch = 20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
